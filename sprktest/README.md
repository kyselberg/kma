# Множення матриць через PySpark на кластері

Простий проект для множення великих матриць (10000x10000) з використанням Apache Spark кластеру та Python.

## Вимоги

- Docker
- Docker Compose

## Запуск

### Запуск кластеру:

```bash
docker-compose up --build
```

Це запустить:
- **spark-master** - головний вузол кластеру (UI доступний на http://localhost:8080)
- **spark-worker** - воркер вузол для обчислень
- **spark-client** - клієнт, який виконує множення матриць

### Перегляд Spark UI

Після запуску відкрийте браузер і перейдіть на:
```
http://localhost:8080
```

Там ви побачите стан кластеру, виконання задач та статистику.

## Що робить програма

1. Створює SparkSession з підключенням до кластеру
2. Генерує дві матриці розміром 10000x10000
3. Множить матриці використовуючи Spark RDD на кластері
4. Виводить час виконання та перші елементи результату
5. Перевіряє результат на невеликій частині через NumPy

## Структура проекту

- `matrix_multiply.py` - основний скрипт з логікою множення матриць
- `Dockerfile` - образ для клієнта (базується на bitnami/spark)
- `docker-compose.yml` - конфігурація Spark кластеру
- `data/` - директорія для даних (створюється автоматично)

## Налаштування

В `docker-compose.yml` можна змінити:
- `SPARK_WORKER_MEMORY` - пам'ять для воркера (за замовчуванням 4g)
- `SPARK_WORKER_CORES` - кількість ядер для воркера (за замовчуванням 2)
- Додати більше воркерів, скопіювавши секцію `spark-worker`

## Примітки

- Для матриць 10000x10000 обчислення може зайняти багато часу
- Моніторинг виконання доступний через Spark UI на порту 8080
- Результат зберігається в пам'яті, для великих матриць можна додати збереження на диск

Звіт про результати проєкту з порівнянням Plain vs Partitioned vs Sharded (PostgreSQL)

1) Огляд і мета
Проєкт досліджує три варіанти організації таблиці подій у PostgreSQL для аналітичних і OLTP-навантажень:
- Plain: одна «плоска» таблиця без партицій.
- Variant A: одна інстанція з RANGE-партиціюванням по місяцях.
- Variant B: координаційний вузол із postgres_fdw і двома шарами (HASH-шардинг за tenant_id).

Цілі:
- Порівняти продуктивність типових запитів: пошук за id, підрахунки в часових діапазонах, tenant-обмежені запити, ролап по місяцях, невеликі вставки.
- Оцінити експлуатаційні аспекти: прюнінг, масштабованість, простота обслуговування.

2) Сетап інфраструктури
Інфраструктура PostgreSQL підіймається через Docker Compose
Сервіси які підіймав:
- Проста БД
- Партиційована БД
- Шард БД 1
- Шард БД 2
- Координатор

Схема таблиці яка юзалась скрізь і партиційонувалась виглядає ось так:
app.events_plain (
    id uuid PRIMARY KEY,
    tenant_id uuid NOT NULL,
    occurred_at timestamptz NOT NULL,
    payload jsonb NOT NULL DEFAULT '{}'::jsonb
);

3) Схеми та налаштування кожної БД
3.1) Звичайна БД без партицій
- Таблиця: `app.events_plain`
- Індекси:
  - idx_events_plain_occurred_at ON (occurred_at)
  - idx_events_plain_tenant ON (tenant_id)
  - idx_events_plain_tenant_time ON (tenant_id, occurred_at)

3.2) Одна інстанція БД, RANGE партиції по місяцю
- Батьківська таблиця: `app.events` PARTITION BY RANGE (occurred_at).
  - PRIMARY KEY (id, occurred_at) — включає ключ партиціювання для коректної унікальності по дочірніх.
- Функція `app.create_month_partition(p_month_start date)` автоматично створює дочірню партицію `app.events_YYYY_MM` для діапазону [початок місяця; початок наступного місяця) і створює локальні індекси:
  - (occurred_at)
  - (tenant_id)
  - (tenant_id, occurred_at)
- Pre-warm партицій: анонімний DO-блок створює партиції від ~-30 місяців до +6 місяців від поточного моменту.
- Партиція за замовчуванням: `app.events_default` зі своїми індексами (occurred_at, tenant_id, (tenant_id, occurred_at)).
- Особливості:
  - Прюнінг партицій за фільтром по `occurred_at`.
  - Для lookup суто за `id` оптимальний шлях — знати очікуваний місяць і одразу таргетити відповідну партицію; інакше планеру доводиться розглядати багато child-таблиць.

3.3) Координатор з postgres_fdw і двома шардами
- На шардах (shard1, shard2):
  - Локальна таблиця `app.events_shard`
  - Локальні індекси: (occurred_at), (tenant_id), (tenant_id, occurred_at).
- На координаторі:
  - Розширення `postgres_fdw`.
  - Визначені FOREIGN SERVER `shard1`, `shard2` і користувацькі мапінги (user app/password app).
  - Батьківська таблиця `app.events_sharded` PARTITION BY HASH (tenant_id).
  - Описані foreign partitions:
    - `app.events_mod2_r0` на сервері shard1, що дивиться на `app.events_shard` (schema app) — ATTACH PARTITION з MODULUS 2, REMAINDER 0.
    - `app.events_mod2_r1` на сервері shard2 — ATTACH PARTITION з MODULUS 2, REMAINDER 1.
- Особливості:
  - Пушдаун фільтрів і агрегацій залежить від можливостей FDW та планера; tenant-фільтр добре рулить трафік на один шар, проте чисто часові фільтри часто проганяються через обидва foreign scans із подальшою фільтрацією локально на координаторі.

4) Генерація та завантаження даних
 Дані згенеровані для проміжку 2024-01 - 2025-12
 На кожен місяць випало 500_000 записів

5) Перевірки коректності
- Перевірка одного інстанса з партиціями:
  - Вставки у декілька місяців;
  - листинг дочірніх партицій і підрахунок рядків по кожній
- Перевірка координатора з шардами:
  - Вставка ~50 рядків з різними tenant_id;
  - групування з tableoid показує розподіл по foreign partitions;
  - запити на shard1 і shard2 напряму.

6) Бенчмарки: методика
- Набір запитів і мотиви:
  1) Point lookup за id — селективний OLTP-запит; оцінює швидкість первинного ключа/роутингу.
  2) Time-range count за один місяць — перевіряє користь прюнінгу й паралелізму.
  3) Tenant-scoped count за 3 місяці — перевіряє (tenant_id, occurred_at) індекси і партиційний прюнінг.
  4) Ролап по місяцях за рік — важкий аналітичний скан із сортуванням/агрегацією.
  5) INSERT 10 рядків — оцінка оверхеду маршрутизації/тригерів/FDW.

7) Результати бенчмарків (узагальнення)
Контекст датасету: ~500 тис/місяць, 24 місяці ≈ 12 млн рядків, ~1000 тенантів.
Нижче — узагальнені "Execution Time"
- Пошук за id:
  - Plain: ~1.2 ms
  - Variant A: ~1.7 ms
  - Variant B: ~110 ms
- count(*) за останній місяць (тільки фільтр часу):
  - Plain: ~37 ms
  - Variant A: ~87 ms
  - Variant B: ~23.2 с
- count(*) для 1 тенанта за останні 3 місяці (tenant_id + час):
  - Plain: ~2.5 ms
  - Variant A: ~0.48 ms
  - Variant B: ~44.9 ms
- Ролап за рік (GROUP BY month):
  - Plain: ~6.23 с
  - Variant A: ~3.00 с
  - Variant B: ~20.0 с
- INSERT 10 рядків:
  - Plain: ~21.7 ms
  - Variant A: ~21.1 ms
  - Variant B: ~14.0 ms

Ключові спостереження з планів:
- Plain
  - Ролап за рік: Index Only Scan по (occurred_at) з великим зовнішнім Merge Sort, спіл ~141 МБ. Дуже швидкий для селективних index-only сканів і простих вставок.
- Variant A
  - Прюнінг до 1 партиції на «місячних» фільтрах; Parallel Append + HashAggregate на ролапі — дає ~2× виграш проти Plain. Tenant-скоплений підрахунок стає ще швидшим завдяки локальним індексам у відповідних партиціях. Lookup лише за id повільніший за Plain, якщо не звузити партицію.
- Variant B
  - На читаннях часто видно обмежений pushdown: часові фільтри виконуються після витягування великого обсягу рядків із обох foreign partitions; звідси високі латентності на time-only і агрегаційних запитах. Вставки — найшвидші, координатор ефективно роутить на шари.

8) Висновки і рекомендації
- Для робочих навантажень із часовими та tenant-обмеженими запитами базовим вибором має бути Variant A (місячні RANGE-партиції):
  - Виграші в аналітичних і фільтрованих запитах, зручне керування retention через дроп партицій, хороший паралелізм.
  - Рекомендації з тюнінгу: розглянути BRIN на `occurred_at` у child-таблицях для purely time-only сканів; за потреби додати часткові індекси на «гарячі» місяці; перевірити параметри планера (`effective_cache_size`, `random_page_cost`). Для lookup за `id` — або знати місяць, або мати допоміжну маршрутизаційну структуру.
- Plain підходить, якщо домінують lookup-и за id і датасет відносно невеликий; простіше в обслуговуванні, інколи швидше на строго селективних запитах.
- Variant B (FDW-шардинг) у поточній конфігурації поступається на читаннях через слабкий pushdown, але має перевагу на вставках і масштабується за кількістю шардів. Для конкурентної швидкодії на аналітиці:
  - Формулювати запити так, щоб координатор міг пушдаунити і tenant, і час, уникати складних виразів.
  - Впевнитися у наявності та актуальності індексів на шардах (`occurred_at`, `(tenant_id, occurred_at)`), ввімкнути `use_remote_estimate`, підібрати `fetch_size`.
  - Розглянути спеціалізовані рішення (Citus/Greenplum тощо) для розподіленого планування складних агрегацій.
